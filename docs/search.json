[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Webscraping Tutorial",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "1  Introduction and Purpose",
    "section": "",
    "text": "Welcome to the Web Scraping course, where we embark on a journey from the basics of web scraping to the development of a fully automated, interactive web application. This course is designed to provide you with a solid foundation in web scraping, combined with practical skills in Python programming, automation with GitHub Actions, and building dynamic web apps using Streamlit.\n\n1.0.0.1 Course Objectives:\nBy the end of this course, you will be able to:\n\nSet Up a Python Environment:\nLearn how to create and manage a virtual environment using venv, ensuring that your web scraping projects are isolated and dependencies are handled efficiently. This step is crucial for maintaining clean, reproducible, and conflict-free projects.\nScrape Websites Efficiently:\nMaster the essential techniques for web scraping using Python libraries such as requests and BeautifulSoup. You’ll start by scraping simple websites, like example.com, and gradually move to more complex tasks, such as extracting structured data from tables on websites like the SARS exchange rates page.\nInspect and Analyze Web Pages:\nDevelop skills in inspecting web pages using tools like SelectorGadget, enabling you to identify the correct HTML elements and CSS selectors to target for scraping. Understanding the structure of a webpage is key to extracting the right data.\nAutomate Web Scraping Tasks:\nAutomate the scraping process by setting up a GitHub repository and creating a GitHub Action with a CRON job. This will allow you to schedule your scraper to run at specific intervals, ensuring your data remains current without manual intervention.\nBuild an Interactive Web Application:\nFinally, you’ll use Streamlit to build a web application that leverages your scraped data. In this course, you’ll create a currency conversion tool that updates in real-time with data scraped from the SARS website, demonstrating how scraping can be integrated into a practical, user-friendly tool.\n\n\n\n1.0.0.2 Course Structure:\nThis course is divided into the following sections:\n\nGetting Started:\n\nSet up your development environment with a virtual environment and install the necessary Python libraries (requests, BeautifulSoup).\nDetailed instructions are provided for both Windows and MacOS users.\n\nScraping example.com:\n\nA step-by-step guide to scraping a simple webpage.\nIntroduction to using SelectorGadget for identifying HTML elements to scrape.\n\nScraping a Table from the SARS Website:\n\nLearn how to extract structured data from tables and save it locally.\nTroubleshoot common issues in scraping table data.\n\nAutomating Scraping with GitHub Actions:\n\nAutomate your scraper to run on a schedule using GitHub Actions and CRON jobs.\nMonitor and manage the automated process effectively.\n\nBuilding a Streamlit App:\n\nDevelop an interactive Streamlit app for currency conversion, using the data you’ve scraped.\nFocus on creating a user-friendly interface and ensuring app responsiveness.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote: This course is designed to take you from the basics of web scraping to building an interactive application that automates scraping tasks and leverages the data in a practical tool. Whether you are a beginner in web scraping or looking to enhance your skills, this course will guide you through every step of the process.\n\n\n\n\n1.0.1 Getting the Most Out of This Course\n\nHands-On Learning:\nEach section is designed with practical exercises. You’ll be writing and running Python code, inspecting web pages, and ultimately building a real-world application.\nBuild as You Learn:\nThe course is structured to incrementally build up to a final project. By the end, you’ll not only have the knowledge but also a complete, functional app to showcase your new skills.\nStay Curious:\nWeb scraping is a dynamic field with constantly evolving techniques and tools. Throughout the course, you’ll find links to further reading and advanced topics. Use these resources to deepen your understanding and explore new possibilities.\n\n\nWith that introduction, you’re now ready to dive into the first section: Getting Started. Let’s set up your environment and get you ready for the exciting tasks ahead!"
  },
  {
    "objectID": "02-virtual-envs.html",
    "href": "02-virtual-envs.html",
    "title": "2  Getting Started: Setting Up a Virtual Environment and Installing Packages",
    "section": "",
    "text": "In this section, we will cover the essential steps to set up your Python development environment. Setting up a virtual environment ensures that your projects are isolated, allowing you to manage dependencies effectively without affecting your global Python installation. This is particularly important for maintaining a clean and conflict-free workspace.\n\n2.0.0.1 1. Setting Up a Virtual Environment with venv\nA virtual environment in Python is an isolated environment that allows you to install packages and dependencies specific to your project without interfering with other projects or the global Python installation.\n\n2.0.0.1.1 1.1 Setting Up on MacOS\nFollow these steps to create and activate a virtual environment on MacOS:\n\nOpen Terminal:\nStart by opening the Terminal application on your Mac.\nNavigate to Your Project Directory:\nUse the cd command to navigate to the directory where you want to create your project.\ncd path/to/your/project-directory\nCreate a Virtual Environment:\nUse the following command to create a virtual environment. Replace myenv with the name you want for your virtual environment.\npython3 -m venv myenv\nActivate the Virtual Environment:\nAfter creating the virtual environment, you need to activate it. Run the following command:\nsource myenv/bin/activate\nOnce activated, you’ll see the name of your virtual environment (e.g., (myenv)) appear at the beginning of your command line prompt, indicating that the environment is active.\nDeactivate the Virtual Environment:\nWhen you are done working in the virtual environment, deactivate it by running:\ndeactivate\n\n\n\n2.0.0.1.2 1.2 Setting Up on Windows\nFor Windows users, the process is slightly different:\n\nOpen Command Prompt or PowerShell:\nYou can use either Command Prompt or PowerShell to set up your virtual environment.\nNavigate to Your Project Directory:\nUse the cd command to navigate to your project directory.\ncd path\\to\\your\\project-directory\nCreate a Virtual Environment:\nCreate a virtual environment using the following command. Replace myenv with your chosen environment name.\npython -m venv myenv\nActivate the Virtual Environment:\nTo activate the virtual environment, run:\n\nFor Command Prompt:\nmyenv\\Scripts\\activate\nFor PowerShell:\n.\\myenv\\Scripts\\Activate.ps1\n\nAfter activation, you should see (myenv) at the beginning of your prompt, indicating that the virtual environment is active.\nDeactivate the Virtual Environment:\nTo deactivate the environment when you are done, simply run:\ndeactivate\n\n\n\n\n\n\n\n\nTip: Virtual Environment\n\n\n\nSetting up a virtual environment ensures that your project dependencies are isolated and easy to manage. This isolation prevents conflicts between different projects and keeps your global Python environment clean.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWarning: Always remember to activate your virtual environment before installing any packages to ensure they are installed within the environment and not globally.\n\n\n\n\n\n\n2.0.0.2 2. Installing Packages with pip\nOnce your virtual environment is activated, you can install the necessary packages using pip, Python’s package manager. We’ll start by installing two essential libraries for web scraping: requests and BeautifulSoup4.\n\n2.0.0.2.1 2.1 Installing Packages\n\nEnsure Your Virtual Environment is Activated:\nBefore installing any packages, make sure your virtual environment is active. If you see (myenv) at the beginning of your command line, you’re good to go.\nInstall requests and BeautifulSoup4:\nUse the following command to install both packages:\npip install requests beautifulsoup4\nThis command tells pip to download and install the requests library, which allows you to send HTTP requests, and BeautifulSoup4, a library for parsing HTML and XML documents.\nVerify the Installation:\nAfter installation, you can verify that the packages were installed correctly by listing the installed packages:\npip list\nYou should see requests and beautifulsoup4 in the list of installed packages.\n\n\n\n2.0.0.2.2 2.2 Installing Specific Versions of Packages\nIf you need to install a specific version of a package, you can specify the version number:\npip install requests==2.26.0\nThis command will install version 2.26.0 of the requests package. Specifying versions can be useful when working on projects that require a particular version of a library.\n\n\n2.0.0.2.3 2.3 Freezing Requirements\nTo make it easier to recreate the same environment later or share with others, you can “freeze” your environment’s current state to a requirements.txt file:\npip freeze &gt; requirements.txt\nThis file lists all the packages and their versions currently installed in your environment. Later, you or someone else can recreate the same environment by running:\npip install -r requirements.txt\n\n\n\n\n\n\n\nNote\n\n\n\nNote: Keeping your requirements.txt file up to date ensures that others can easily set up an identical environment for your project.\n\n\n\nWith your virtual environment set up and the necessary packages installed, you’re ready to start working on your web scraping project. In the next section, we’ll begin by scraping data from a simple webpage, example.com, and exploring how to use tools like SelectorGadget to identify the elements you need."
  },
  {
    "objectID": "03-scraping-basics.html",
    "href": "03-scraping-basics.html",
    "title": "3  Scraping Basics and Scraping example.com",
    "section": "",
    "text": "In this section, we will dive into the fundamentals of web scraping and provide a hands-on example by scraping a simple webpage, example.com. This exercise will help you understand how to send HTTP requests, parse HTML content, and extract specific data from a webpage.\n\n3.0.0.1 1. What is Web Scraping?\nWeb scraping is the process of programmatically extracting information from websites. It involves fetching the content of a webpage, parsing the HTML, and selecting the data you need. Web scraping is a powerful tool for gathering data from the web for various applications, such as data analysis, machine learning, and automation.\nHowever, it’s important to note that not all websites allow scraping, and some have measures in place to prevent it. Always check a website’s robots.txt file or terms of service to ensure you are scraping responsibly and within legal boundaries.\n\n\n\n\n\n\n\nUnderstanding robots.txt\n\n\n\nImportant: Always respect the terms of service of the website you are scraping. Be mindful of legal and ethical considerations when scraping data, and avoid overloading servers with frequent requests.\nA robots.txt file is a simple text file placed at the root of a website to provide instructions to web crawlers and bots about which parts of the website they are allowed to access and index. When a web scraper or search engine bot visits a site, it typically checks the robots.txt file first to see what it is permitted to scrape.\n\n3.0.1 Key Points:\n\nLocation: The robots.txt file is located at the root of a website, usually accessible via http://example.com/robots.txt.\nSyntax: The file consists of directives that allow or disallow access to specific parts of the site. For example:\nUser-agent: *\nDisallow: /private-directory/\nThis example tells all user agents (crawlers) that they are not allowed to access /private-directory/.\nRespecting robots.txt: While the directives in robots.txt are not legally binding, ethical web scraping practices dictate that you should respect them. Ignoring these directives could lead to legal repercussions or your IP address being blocked by the website.\nLimitations: Keep in mind that robots.txt is a voluntary protocol. Some bots might choose to ignore it, and it cannot prevent all forms of access.\n\nBefore scraping a website, always check the robots.txt file to ensure your activities align with the website’s guidelines.\n\n\n\n\n\n\n3.0.1.1 2. Overview of Scraping Process\nThe general process for web scraping involves the following steps:\n\nSending an HTTP Request:\nUse a Python library like requests to send an HTTP request to the server hosting the website.\nRetrieving the Response:\nThe server returns an HTTP response, typically containing the HTML of the webpage.\nParsing the HTML:\nUse a library like BeautifulSoup to parse the HTML and navigate the structure of the webpage.\nIdentifying and Extracting Data:\nLocate the specific HTML elements (e.g., &lt;div&gt;, &lt;p&gt;, &lt;a&gt;, etc.) that contain the data you want and extract it.\nSaving or Processing the Data:\nStore the extracted data in a suitable format, such as a CSV file or a database, for further analysis or use.\n\n\n\n\n3.0.1.2 3. Scraping example.com: A Hands-On Example\nTo get started with web scraping, we’ll walk through a simple example using example.com, a placeholder domain provided by IANA (Internet Assigned Numbers Authority). While this website contains very basic content, it is perfect for demonstrating the fundamental concepts of web scraping.\n\n3.0.1.2.1 3.1 Sending an HTTP Request\nFirst, we’ll send an HTTP GET request to example.com using the requests library.\nimport requests\n\n# Send a GET request to the webpage\nurl = \"http://example.com\"\nresponse = requests.get(url)\n\n# Check if the request was successful\nif response.status_code == 200:\n    print(\"Successfully fetched the webpage!\")\nelse:\n    print(f\"Failed to fetch the webpage. Status code: {response.status_code}\")\nIn this code snippet: - We import the requests library. - We define the URL for example.com. - We send a GET request to fetch the webpage. - We check if the request was successful by inspecting the status code (200 indicates success).\n\n\n3.0.1.2.2 3.2 Parsing the HTML\nNext, we need to parse the HTML content of the page using BeautifulSoup to make it easier to navigate and extract specific elements.\nfrom bs4 import BeautifulSoup\n\n# Parse the HTML content\nsoup = BeautifulSoup(response.text, 'html.parser')\n\n# Print the title of the webpage\ntitle = soup.title.string\nprint(f\"Page Title: {title}\")\nIn this snippet: - We import BeautifulSoup from the bs4 library. - We parse the HTML content of the response using BeautifulSoup with the HTML parser. - We extract and print the title of the webpage using the &lt;title&gt; tag.\n\n\n3.0.1.2.3 3.3 Extracting Specific Data\nLet’s extract specific elements from the page. For example, we’ll extract all the paragraphs (&lt;p&gt; tags) on the page.\n# Extract and print all paragraphs\nparagraphs = soup.find_all('p')\nfor idx, paragraph in enumerate(paragraphs, start=1):\n    print(f\"Paragraph {idx}: {paragraph.text}\")\nIn this snippet: - We use soup.find_all('p') to find all paragraph elements on the page. - We loop through each paragraph and print its content.\n\n\n3.0.1.2.4 3.4 Summary of Extracted Data\nFor example.com, the content is very simple. The title of the page is “Example Domain,” and the body contains just a few paragraphs explaining the purpose of the domain. By following these steps, you’ve successfully scraped and extracted data from a webpage!\n\n\n\n\n\n\n\nTip: Inspecting Web Pages\n\n\n\nUse your web browser’s developer tools (usually accessible by right-clicking on the page and selecting “Inspect”) to explore the HTML structure of a webpage. This helps in identifying the tags and classes associated with the data you want to scrape.\n\n\n\n\n\n\n3.0.1.3 4. Using SelectorGadget to Identify HTML Elements\nTo scrape more complex websites, it’s essential to accurately identify the HTML elements that contain the data you need. SelectorGadget is a browser extension that simplifies this process by allowing you to click on elements and automatically generating the correct CSS selector.\n\n3.0.1.3.1 4.1 Installing SelectorGadget\n\nChrome:\nInstall the SelectorGadget extension from the Chrome Web Store.\nFirefox:\nInstall the SelectorGadget bookmarklet by visiting the SelectorGadget website.\n\n\n\n3.0.1.3.2 4.2 Using SelectorGadget\n\nActivate SelectorGadget:\nClick the SelectorGadget icon in your browser to activate it.\nSelect Elements:\nHover over elements on the webpage to highlight them. Click to select an element. SelectorGadget will generate a CSS selector that you can use in your scraping script.\nRefine Selection:\nIf the initial selection is too broad or too narrow, you can click on additional elements to refine the selector. The goal is to create a selector that precisely targets the data you want to extract.\n\n\n\n3.0.1.3.3 4.3 Applying the Selector in Your Code\nOnce you have the CSS selector from SelectorGadget, you can use it in your BeautifulSoup code to target specific elements.\n# Example: Using a CSS selector to find elements\nselected_elements = soup.select('css-selector-from-selectorgadget')\nfor element in selected_elements:\n    print(element.text)\nReplace 'css-selector-from-selectorgadget' with the actual selector provided by SelectorGadget.\n\n\n\n\n3.0.2 Conclusion\nIn this section, you’ve learned the basics of web scraping, including how to send HTTP requests, parse HTML content, and extract specific data. You’ve also seen how to use SelectorGadget to simplify the process of identifying HTML elements on more complex webpages.\nWith these foundational skills, you are now ready to tackle more advanced scraping tasks. In the next section, we will explore scraping structured data from a table on the SARS website and saving it for further use."
  },
  {
    "objectID": "04-scraping-sars.html",
    "href": "04-scraping-sars.html",
    "title": "4  Scraping a Table from the SARS Website",
    "section": "",
    "text": "In this section, you will learn how to scrape data from a table on the SARS website, specifically the exchange rates table, and save this data to a pandas DataFrame. Once the data is in a DataFrame, we will also demonstrate how to save it as a JSON file for further use.\n\n4.0.0.1 1. Inspecting the Webpage\nThe first step in web scraping is to inspect the webpage to identify the structure and the elements you want to extract. The SARS Rates of Exchange page contains a table with exchange rates for various currencies. This table is defined using standard HTML &lt;table&gt; tags, with each row (&lt;tr&gt;) containing data for a different currency.\nTo scrape this table, we’ll focus on extracting the following data for each currency: - Country name - Abbreviation - Currency name - Exchange rate\nThe HTML structure of the table includes: - A table element with class table table-bordered gvExt. - Each row (&lt;tr&gt;) contains several cells (&lt;td&gt;), where: - The first cell contains the country flag (which we will ignore). - The second cell contains the country name. - The third cell contains the currency abbreviation. - The fourth cell contains the currency name. - The fifth cell contains the exchange rate.\n\n\n\n4.0.0.2 2. Setting Up Your Environment\nBefore we start scraping, ensure that you have the necessary Python libraries installed in your virtual environment. We will use requests to fetch the webpage content, BeautifulSoup to parse the HTML, and pandas to store the data in a DataFrame.\npip install requests beautifulsoup4 pandas\n\n\n\n4.0.0.3 3. Writing the Scraper\nBelow is the step-by-step guide to scraping the exchange rates table from the SARS website:\n\n4.0.0.3.1 3.1 Importing the Required Libraries\nStart by importing the necessary libraries.\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\n\n4.0.0.3.2 3.2 Sending a GET Request to Fetch the Webpage\nWe’ll send an HTTP GET request to retrieve the HTML content of the webpage.\nurl = \"https://tools.sars.gov.za/rex/Rates/Default.aspx\"\nresponse = requests.get(url)\n\nif response.status_code == 200:\n    print(\"Successfully fetched the webpage!\")\nelse:\n    print(f\"Failed to fetch the webpage. Status code: {response.status_code}\")\n\n\n4.0.0.3.3 3.3 Parsing the HTML with BeautifulSoup\nNext, parse the HTML content using BeautifulSoup.\nsoup = BeautifulSoup(response.text, 'html.parser')\n\n\n4.0.0.3.4 3.4 Locating the Table and Extracting Data\nWe will locate the table by its class name and then iterate over the rows to extract the relevant data.\n# Locate the table\ntable = soup.find('table', {'class': 'table table-bordered gvExt'})\n\n# Prepare lists to store the extracted data\ncountries = []\nabbreviations = []\ncurrencies = []\nrates = []\n\n# Loop through each row in the table\nfor row in table.find_all('tr')[1:]:  # Skip the header row\n    cells = row.find_all('td')\n    country = cells[1].text.strip()\n    abbreviation = cells[2].text.strip()\n    currency = cells[3].text.strip()\n    rate = float(cells[4].text.strip())\n    \n    # Append the data to the lists\n    countries.append(country)\n    abbreviations.append(abbreviation)\n    currencies.append(currency)\n    rates.append(rate)\n\n\n4.0.0.3.5 3.5 Creating a Pandas DataFrame\nNow that we have the data, we can create a pandas DataFrame to organize it.\n# Create a DataFrame\ndf = pd.DataFrame({\n    'Country': countries,\n    'Abbreviation': abbreviations,\n    'Currency': currencies,\n    'Rate': rates\n})\n\n# Display the DataFrame\nprint(df)\n\n\n4.0.0.3.6 3.6 Saving the Data to a JSON File\nFinally, let’s save the DataFrame to a JSON file for further use.\n# Save the DataFrame to a JSON file\ndf.to_json('exchange_rates.json', orient='records', indent=4)\n\nprint(\"Data has been saved to exchange_rates.json\")\n\n\n\n\n4.0.1 4. Summary\nIn this section, you learned how to: 1. Inspect a webpage to locate a table for scraping. 2. Write a Python script to scrape data from the table using requests and BeautifulSoup. 3. Store the scraped data in a pandas DataFrame. 4. Save the DataFrame to a JSON file.\nThis basic approach to scraping tables can be applied to many other websites, allowing you to automate the extraction of tabular data for analysis, reporting, or other applications.\n\n\n\n\n\n\n\nTip: Handling Changes in Webpage Structure\n\n\n\nWebsites can change their HTML structure over time, which may break your scraper. To maintain your scraping scripts, periodically check the structure of the pages you scrape and update your code as needed.\n\n\nIn the next section, we will explore how to automate this scraping process using GitHub Actions, enabling you to keep your data up-to-date without manual intervention."
  }
]